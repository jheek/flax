{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import dataclasses\n",
    "from typing import Any, Callable, Iterable, Optional, Tuple, Type\n",
    "from flax import nn\n",
    "from flax.nn import initializers\n",
    "from flax.core.scope import Scope\n",
    "from flax.core import scope\n",
    "from jax import numpy as jnp\n",
    "import functools\n",
    "\n",
    "from flax.core.frozen_dict import freeze\n",
    "\n",
    "@dataclass\n",
    "# TODO: Document that any class that extends from Module must add\n",
    "#   name = Optional[None]\n",
    "class Module:\n",
    "  parent: Optional[Type[\"Module\"]]\n",
    "  \n",
    "  # TODO: Use Dataclass \"hidden\" attributes that don't appear on __init__.\n",
    "  # Then remove all use of hasattr\n",
    "  \n",
    "  @classmethod\n",
    "  def toplevel(cls, *args, rngs=None, variables=None, mutable=False, **kwargs):\n",
    "    # TODO: Think about the fact that `rngs` and `params` live on args\n",
    "    # and kwargs\n",
    "    if rngs is None:\n",
    "      rngs = {}\n",
    "    if variables is None:\n",
    "      variables = {'param': {}}\n",
    "    module = cls(None, *args, **kwargs)  # first argument is `parent` dataclass attr\n",
    "    module._scope = Scope(variables, rngs=rngs)    \n",
    "\n",
    "    # QUESTION: Make sure to unfreeze after calling _recurse so that you don't need\n",
    "    # to set params as mutable during construction time...?\n",
    "    new_variables = scope._unfreeze_variables(variables, mutable)\n",
    "    module._scope.variables = new_variables\n",
    "\n",
    "    return module\n",
    "\n",
    "  def on_attached(self):\n",
    "    pass\n",
    "  \n",
    "  def autonamed(self):\n",
    "    if not hasattr(self, '_autonamed'):\n",
    "      self._autonamed = {}\n",
    "    return self._autonamed\n",
    "\n",
    "  def submodules(self):\n",
    "    if not hasattr(self, '_submodules'):\n",
    "      self._submodules = {}\n",
    "    return self._submodules\n",
    "\n",
    "  def _ensure_has_name(self):\n",
    "    if not hasattr(self, 'name') or self.name is None:\n",
    "      if not hasattr(self.parent, '_in_autonames') or not self.parent._in_autonames:\n",
    "        raise ValueError(\"In order to get autonames, must decorate method with @autonames\")\n",
    "\n",
    "      self.name = \"{}/{}\".format(\n",
    "        self.__class__.__name__, \n",
    "        str(len(self.parent.autonamed())))\n",
    "      self.parent.autonamed()[self.name] = self\n",
    "  \n",
    "  def __setattr__(self, name, value):\n",
    "    # GOTCHA: This is very brittle.\n",
    "    super().__setattr__(name, value)\n",
    "\n",
    "    if name != 'parent':\n",
    "      def _recurse(x):\n",
    "        if isinstance(x, Module):\n",
    "          x._ensure_has_name()\n",
    "        elif isinstance(x, list):\n",
    "          # TODO: Make this work on iterables?\n",
    "          for submodule in x:\n",
    "            _recurse(submodule)\n",
    "        # TODO: Also support dicts?\n",
    "\n",
    "      _recurse(value)\n",
    "\n",
    "  \n",
    "  def _init_scope(self):\n",
    "    if self.parent is None:\n",
    "      # NOTE: This error also happens if you try to initialize parameters\n",
    "      # during __post_init__. Try to catch this.\n",
    "      raise ValueError(\n",
    "        'Trying to create a module instance at the top-level? '\n",
    "        'Use, e.g. `MyModule.toplevel(...)`')\n",
    "\n",
    "    self._ensure_has_name()\n",
    "\n",
    "    self.parent.submodules()[self.name] = self    \n",
    "\n",
    "    if not hasattr(self.parent, '_scope'):\n",
    "      self.parent._init_scope()\n",
    "\n",
    "    # TODO: Make scopes know of sublists, then don't call\n",
    "    # push by name here.\n",
    "    self._scope = self.parent._scope.push(self.name)\n",
    "    \n",
    "  def scope(self):\n",
    "    if not hasattr(self, '_scope'):\n",
    "      self._init_scope()\n",
    "    return self._scope\n",
    "    \n",
    "  def variables(self):\n",
    "    return self.scope().variables\n",
    "\n",
    "  def param(self, name, init_fun, shape):\n",
    "    return self.scope().param(name, init_fun, shape)\n",
    "\n",
    "\n",
    "def autonames(fun, prefix=''):\n",
    "  @functools.wraps(fun)\n",
    "  def wrapped(self, *args, **kwargs):\n",
    "    if hasattr(self, '_autonames_fun') and self._autonames_fun != fun:\n",
    "      raise Error(\n",
    "        \"Can't only use @autonames on one method. \"\n",
    "        \"Use @method_autonames for additional autonaming scopes.\")      \n",
    "    self._autonames_fun = fun\n",
    "\n",
    "    # \"Rewind\" the autonaming process\n",
    "    self.autonamed().clear()\n",
    "\n",
    "    if hasattr(self, '_in_autonames') and self._in_autonames:\n",
    "      raise Error(\"Can't nest `autonames`-decorated function calls\")\n",
    "    \n",
    "    self._in_autonames = True\n",
    "    try:\n",
    "      return fun(self, *args, **kwargs)\n",
    "    finally:\n",
    "      self._in_autonames = False\n",
    "\n",
    "  return wrapped\n",
    "\n",
    "@dataclass\n",
    "class Dense(Module):\n",
    "  features: int\n",
    "  bias: bool = True\n",
    "  kernel_init: Callable = initializers.lecun_normal()\n",
    "  bias_init: Callable = initializers.zeros\n",
    "  name: str = None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    kernel = self.param('kernel', self.kernel_init, (x.shape[-1], self.features))\n",
    "    x = jnp.dot(x, kernel)\n",
    "    if self.bias:\n",
    "      x = x + self.param('bias', self.bias_init, (self.features,))\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we make this a better error message?\n",
    "# Dense(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-2.3335357,  1.6050829, -2.0810487],\n",
       "             [-2.3335357,  1.6050829, -2.0810487],\n",
       "             [-2.3335357,  1.6050829, -2.0810487]], dtype=float32)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: It would be nice to make this throw an error,\n",
    "# but how? I'd like to avoid requiring people to wrap /all/\n",
    "# methods in a decorator (or the similar metaclass approach with\n",
    "# hk.transparent).\n",
    "#\n",
    "# QUESTION: Can we resolve this by inspecting stack traces \n",
    "# when constucting modules, or when using them? Only during\n",
    "# \"DEBUG\" runs\n",
    "@dataclass\n",
    "class TryReusingByNameCausesError(Module):\n",
    "  name: Optional[str]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return Dense(self, 3, name=\"foo\")(x) + Dense(self, 3, name=\"foo\")(x)\n",
    "  \n",
    "try_reuse = TryReusingByNameCausesError.toplevel(3, rngs={'param': jax.random.PRNGKey(0)}, mutable=['param'])\n",
    "try_reuse(np.ones((3, 3)))\n",
    "try_reuse(np.ones((3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]]\n",
      "{'param': {'kernel': DeviceArray([[ 0.32717842,  0.05599118,  0.17998298],\n",
      "             [-0.12294921,  0.7071209 ,  0.28972217],\n",
      "             [ 0.1373136 , -0.02852853, -0.62830234]], dtype=float32), 'bias': DeviceArray([0., 0., 0.], dtype=float32)}}\n",
      "[[ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]\n",
      " [ 0.3415428   0.73458356 -0.15859717]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "# init\n",
    "d = Dense.toplevel(3, rngs={'param': jax.random.PRNGKey(0)}, mutable=['param'])\n",
    "print(d(np.ones((3, 3))))\n",
    "print(d.variables())\n",
    "\n",
    "# Can call method twice on the same instance.\n",
    "print(d(np.ones((3, 3))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setattr features 3\n",
      "setattr bias True\n",
      "setattr kernel_init <function variance_scaling.<locals>.init at 0x132853cb0>\n",
      "setattr bias_init <function zeros at 0x1118000e0>\n",
      "setattr name None\n",
      "setattr _scope <flax.core.scope.Scope object at 0x1342fb190>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.3415428 ,  0.73458356, -0.15859717],\n",
       "             [ 0.3415428 ,  0.73458356, -0.15859717],\n",
       "             [ 0.3415428 ,  0.73458356, -0.15859717]], dtype=float32)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply\n",
    "d2 = Dense.toplevel(3, variables=d.variables())\n",
    "d2(np.ones((3, 3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MLP(Module):\n",
    "  widths: Tuple\n",
    "  name: str = None\n",
    "\n",
    "  @autonames\n",
    "  def __call__(self, x):\n",
    "    for width in self.widths[:-1]:\n",
    "      x = nn.relu(Dense(self, width)(x))\n",
    "    x = Dense(self, self.widths[-1])(x)\n",
    "    return x\n",
    "\n",
    "  def params_by_layer_index(self, layer_index):\n",
    "    return list(self.autonamed().values())[layer_index].variables()['param']\n",
    "  \n",
    "  def params_by_layer_name(layer_name):\n",
    "    return self.submodules()[layer_name].variables()['param']\n",
    "    \n",
    "@dataclass\n",
    "class Sequential(Module):\n",
    "  layers: Tuple[Module]\n",
    "  name: str = None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "@dataclass\n",
    "class MLP2(Module):\n",
    "  widths: Tuple\n",
    "  name: str = None\n",
    "\n",
    "  # QUESTION: If you implement __init__ do you need to call super.__init__ with parent and\n",
    "  # name_or_list\n",
    "  \n",
    "  @autonames\n",
    "  def __post_init__(self):\n",
    "    self.layers = [Dense(self, width) for width in self.widths]\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers[:-1]:\n",
    "      x = nn.relu(layer(x))\n",
    "    x = self.layers[-1](x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class AutoEncoder(Module):\n",
    "  encoder_widths: Iterable\n",
    "  decoder_widths: Iterable\n",
    "  in_shape: Tuple = None\n",
    "  name: str = None\n",
    "\n",
    "  # QUESTION: Should only one method be allowed to be wrapped\n",
    "  # in `autonames`?\n",
    "\n",
    "  def __post_init__(self):\n",
    "    self.encoder = self.Encoder(self, 'encoder')\n",
    "    self.decoder = self.Decoder(self, 'decoder')\n",
    "  \n",
    "  def reconstruct(self, x):\n",
    "    return self.decoder(self.encoder(x))\n",
    "  \n",
    "  @dataclass\n",
    "  class Encoder(Module):\n",
    "    name: str\n",
    "    \n",
    "    @autonames\n",
    "    def __call__(self, x):\n",
    "      self.in_shape = x.shape[1:]\n",
    "      # QUESTION: Is this a legitimate use of `self.parent`?\n",
    "      for width in self.parent.encoder_widths[:-1]:\n",
    "        x = nn.relu(Dense(self, width)(x))\n",
    "      z = Dense(self, self.parent.encoder_widths[-1])(x)\n",
    "      return z\n",
    "  \n",
    "  @dataclass\n",
    "  class Decoder(Module):\n",
    "    name: str\n",
    "    \n",
    "    @autonames\n",
    "    def __call__(self, z):\n",
    "      for width in self.parent.decoder_widths[:-1]:\n",
    "        z = nn.relu(Dense(self, width)(z))\n",
    "      x = Dense(self, self.parent.encoder_widths[-1])(z)\n",
    "      # QUESITON: Is this weird? Navigating up then into encoder?\n",
    "      x = x.reshape(x.shape[:-1] + self.parent.encoder.in_shape)\n",
    "      return x\n",
    "\n",
    "@dataclass\n",
    "class AutoEncoder1_5(Module):\n",
    "  encoder_widths: Iterable\n",
    "  decoder_widths: Iterable\n",
    "  in_shape: Tuple = None\n",
    "  name: str = None\n",
    "\n",
    "  # QUESTION: Should only one method be allowed to be wrapped\n",
    "  # in `autonames`?\n",
    "\n",
    "  def encode(self, x):\n",
    "    _ae = self\n",
    "    @dataclass\n",
    "    class Encoder(Module):\n",
    "      name: str = 'encoder'\n",
    "\n",
    "      @autonames\n",
    "      def __call__(self, x):\n",
    "        # QUESTION: Is this a legitimate use of `self.parent`?\n",
    "        for width in _ae.encoder_widths[:-1]:\n",
    "          x = nn.relu(Dense(self, width)(x))\n",
    "        z = Dense(self, _ae.encoder_widths[-1])(x)\n",
    "        return z\n",
    "      \n",
    "    self.in_shape = x.shape[1:]\n",
    "    return Encoder(self)(x)\n",
    "    \n",
    "  def decode(self, x):\n",
    "    _ae = self\n",
    "    @dataclass\n",
    "    class Decoder(Module):\n",
    "      name: str = 'decode'\n",
    "\n",
    "      @autonames\n",
    "      def __call__(self, z):\n",
    "        for width in _ae.decoder_widths[:-1]:\n",
    "          z = nn.relu(Dense(self, width)(z))\n",
    "        x = Dense(self, _ae.encoder_widths[-1])(z)\n",
    "        # QUESITON: Is this weird? Navigating up then into encoder?\n",
    "        x = x.reshape(x.shape[:-1] + _ae.in_shape)\n",
    "        return x\n",
    "\n",
    "    return Decoder(self)(x)\n",
    "\n",
    "  def reconstruct(self, x):\n",
    "    return self.decode(self.encode(x))\n",
    "  \n",
    "  \n",
    "  @dataclass\n",
    "  class Decoder(Module):\n",
    "    name: str\n",
    "    \n",
    "    @autonames\n",
    "    def __call__(self, z):\n",
    "      for width in self.parent.decoder_widths[:-1]:\n",
    "        z = nn.relu(Dense(self, width)(z))\n",
    "      x = Dense(self, self.parent.encoder_widths[-1])(z)\n",
    "      # QUESITON: Is this weird? Navigating up then into encoder?\n",
    "      x = x.reshape(x.shape[:-1] + self.parent.encoder.in_shape)\n",
    "      return x\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class AutoEncoder2(Module):\n",
    "  encoder_widths: Iterable\n",
    "  decoder_widths: Iterable\n",
    "  in_shape: Tuple = None\n",
    "  name: str = None\n",
    "    \n",
    "  def __post_init__(self):\n",
    "    self.encoder = MLP2(self, self.encoder_widths, name='encode')\n",
    "    self.decoder = MLP2(self, self.decoder_widths, name='decode')\n",
    "    \n",
    "  def encode(self, x):\n",
    "    self.in_shape = x.shape[1:]\n",
    "    return self.encoder(x)\n",
    "  \n",
    "  def decode(self, x):\n",
    "    x = self.decoder(x)\n",
    "    x = x.reshape(x.shape[:-1] + self.in_shape)\n",
    "    return x\n",
    "  \n",
    "  def reconstruct(self, x):\n",
    "    return self.decode(self.encode(x))\n",
    "  \n",
    "@dataclass\n",
    "class DAE(Module):\n",
    "  encoder: Module\n",
    "  decoder: Module\n",
    "  \n",
    "  def reconstruction_loss(self, x):\n",
    "    rng = 'foo'\n",
    "    return self.loss(self.apply_noise(rng, x), self.reconstruct(x))\n",
    "\n",
    "  def reconstruct(self, x):\n",
    "    return self.decoder(self.encoder(x))\n",
    "  \n",
    "  def apply_noise(self, rng, x):\n",
    "    return x\n",
    "\n",
    "  def loss(self, inputs, reconstruction):\n",
    "    return np.mean(np.abs(inputs - reconstruction))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]]\n",
      "{'param': {'Dense/0': {'kernel': DeviceArray([[ 0.31915382, -0.76709324,  0.07335479],\n",
      "             [ 0.3674749 ,  0.6602518 , -0.09766117],\n",
      "             [ 0.84561044,  0.16911158,  0.42713115]], dtype=float32), 'bias': DeviceArray([0., 0., 0.], dtype=float32)}, 'Dense/1': {'kernel': DeviceArray([[ 0.1547904 ,  0.00701489,  0.05388883,  0.0586841 ],\n",
      "             [-0.6559397 , -0.5662961 , -0.9021673 ,  0.4098059 ],\n",
      "             [ 0.07156377,  0.72754294, -0.01657445, -1.0881848 ]],            dtype=float32), 'bias': DeviceArray([0., 0., 0., 0.], dtype=float32)}, 'Dense/2': {'kernel': DeviceArray([[-0.532973  , -0.23218885,  0.38628116,  0.31261072,\n",
      "              -0.55765074],\n",
      "             [-0.14478445,  0.83220255, -0.52289605,  0.11223655,\n",
      "               0.41507402],\n",
      "             [-0.62332535,  0.15522319, -0.8609153 ,  0.1192041 ,\n",
      "              -0.84271395],\n",
      "             [ 1.0219636 , -0.0413699 ,  0.39705953, -0.3570713 ,\n",
      "              -0.15464471]], dtype=float32), 'bias': DeviceArray([0., 0., 0., 0., 0.], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP.toplevel([3, 4, 5], rngs={'param': jax.random.PRNGKey(0)}, mutable=['params'])\n",
    "print(mlp(np.ones((3, 3))))\n",
    "print(mlp.variables())\n",
    " \n",
    "# QUESTION: Can you point two models to the same parameter object?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setattr widths [3, 4, 5]\n",
      "setattr name None\n",
      "setattr _autonames_fun <function MLP2.__post_init__ at 0x132885680>\n",
      "setattr _autonamed {}\n",
      "setattr _in_autonames True\n",
      "setattr features 3\n",
      "setattr bias True\n",
      "setattr kernel_init <function variance_scaling.<locals>.init at 0x132853cb0>\n",
      "setattr bias_init <function zeros at 0x1118000e0>\n",
      "setattr name None\n",
      "setattr features 4\n",
      "setattr bias True\n",
      "setattr kernel_init <function variance_scaling.<locals>.init at 0x132853cb0>\n",
      "setattr bias_init <function zeros at 0x1118000e0>\n",
      "setattr name None\n",
      "setattr features 5\n",
      "setattr bias True\n",
      "setattr kernel_init <function variance_scaling.<locals>.init at 0x132853cb0>\n",
      "setattr bias_init <function zeros at 0x1118000e0>\n",
      "setattr name None\n",
      "setattr layers [Dense(parent=MLP2(parent=None, widths=[3, 4, 5], name=None), features=3, bias=True, kernel_init=<function variance_scaling.<locals>.init at 0x132853cb0>, bias_init=<function zeros at 0x1118000e0>, name=None), Dense(parent=MLP2(parent=None, widths=[3, 4, 5], name=None), features=4, bias=True, kernel_init=<function variance_scaling.<locals>.init at 0x132853cb0>, bias_init=<function zeros at 0x1118000e0>, name=None), Dense(parent=MLP2(parent=None, widths=[3, 4, 5], name=None), features=5, bias=True, kernel_init=<function variance_scaling.<locals>.init at 0x132853cb0>, bias_init=<function zeros at 0x1118000e0>, name=None)]\n",
      "setattr name Dense/0\n",
      "setattr name Dense/1\n",
      "setattr name Dense/2\n",
      "setattr _in_autonames False\n",
      "setattr _scope <flax.core.scope.Scope object at 0x1342b21d0>\n",
      "setattr _submodules {}\n",
      "setattr _scope <flax.core.scope.Scope object at 0x134320110>\n",
      "setattr _scope <flax.core.scope.Scope object at 0x134142050>\n",
      "setattr _scope <flax.core.scope.Scope object at 0x1341cdfd0>\n",
      "[[-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]]\n",
      "{'param': {'Dense/0': {'kernel': DeviceArray([[ 0.31915382, -0.76709324,  0.07335479],\n",
      "             [ 0.3674749 ,  0.6602518 , -0.09766117],\n",
      "             [ 0.84561044,  0.16911158,  0.42713115]], dtype=float32), 'bias': DeviceArray([0., 0., 0.], dtype=float32)}, 'Dense/1': {'kernel': DeviceArray([[ 0.1547904 ,  0.00701489,  0.05388883,  0.0586841 ],\n",
      "             [-0.6559397 , -0.5662961 , -0.9021673 ,  0.4098059 ],\n",
      "             [ 0.07156377,  0.72754294, -0.01657445, -1.0881848 ]],            dtype=float32), 'bias': DeviceArray([0., 0., 0., 0.], dtype=float32)}, 'Dense/2': {'kernel': DeviceArray([[-0.532973  , -0.23218885,  0.38628116,  0.31261072,\n",
      "              -0.55765074],\n",
      "             [-0.14478445,  0.83220255, -0.52289605,  0.11223655,\n",
      "               0.41507402],\n",
      "             [-0.62332535,  0.15522319, -0.8609153 ,  0.1192041 ,\n",
      "              -0.84271395],\n",
      "             [ 1.0219636 , -0.0413699 ,  0.39705953, -0.3570713 ,\n",
      "              -0.15464471]], dtype=float32), 'bias': DeviceArray([0., 0., 0., 0., 0.], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLP2.toplevel([3, 4, 5], rngs={'param': jax.random.PRNGKey(0)}, mutable=['params'])\n",
    "print(mlp2(np.ones((3, 3))))\n",
    "print(mlp2.variables())\n",
    " \n",
    "# QUESTION: Can you point two models to the same parameter object?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param': {}}"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': DeviceArray([[ 0.1547904 ,  0.00701489,  0.05388883,  0.0586841 ],\n",
       "              [-0.6559397 , -0.5662961 , -0.9021673 ,  0.4098059 ],\n",
       "              [ 0.07156377,  0.72754294, -0.01657445, -1.0881848 ]],            dtype=float32),\n",
       " 'bias': DeviceArray([0., 0., 0., 0.], dtype=float32)}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.params_by_layer_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]\n",
      " [-0.17117555  0.17427535 -0.070427    0.10287903 -0.03070323]]\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLP2.toplevel([3, 4, 5], variables=mlp.variables())\n",
    "print(mlp2(np.ones((3, 3))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make a clear error if you call AutoEncoder(...) without a parent\n",
    "ae = AutoEncoder.toplevel(\n",
    "  encoder_widths=[3, 3], decoder_widths=[3, 3],\n",
    "  rngs={'param': jax.random.PRNGKey(1)}, mutable=['params']\n",
    ")\n",
    "ae.reconstruct(np.ones((4, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.0273236 , -0.30911958, -0.9112693 ],\n",
       "             [ 0.0273236 , -0.30911958, -0.9112693 ],\n",
       "             [ 0.0273236 , -0.30911958, -0.9112693 ],\n",
       "             [ 0.0273236 , -0.30911958, -0.9112693 ]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make a clear error if you call AutoEncoder(...) without a parent\n",
    "ae2 = AutoEncoder2.toplevel(\n",
    "  encoder_widths=[3, 3], decoder_widths=[3, 3],\n",
    "  rngs={'param': jax.random.PRNGKey(1)}, mutable=['params']\n",
    ")\n",
    "ae2.reconstruct(np.ones((4, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-0.0291461 , -1.0122029 , -0.33748102],\n",
       "             [-0.0291461 , -1.0122029 , -0.33748102],\n",
       "             [-0.0291461 , -1.0122029 , -0.33748102],\n",
       "             [-0.0291461 , -1.0122029 , -0.33748102]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make a clear error if you call AutoEncoder(...) without a parent\n",
    "ae1_5 = AutoEncoder1_5.toplevel(\n",
    "  encoder_widths=[3, 3], decoder_widths=[3, 3],\n",
    "  rngs={'param': jax.random.PRNGKey(1)}, mutable=['params']\n",
    ")\n",
    "ae1_5.reconstruct(np.ones((4, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634],\n",
       "             [-0.01497263, -0.805205  , -0.94335634]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# QUESTION: Does this work?!\n",
    "# should this error? We're connecting submodules of another module into here.\n",
    "# we should either think carefully about what kind of (both good and bad) behavior this\n",
    "# may lead to. Or if we're not sure we can make it raise an Error.\n",
    "dae = DAE.toplevel(\n",
    "  encoder=ae.encoder, decoder=ae.decoder,\n",
    "  rngs={'param': jax.random.PRNGKey(0)}, mutable=['param']\n",
    ")\n",
    "dae.reconstruct(np.ones((4, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "\n",
    "X = np.ones((1, 10))\n",
    "Y = np.ones((5, ))\n",
    "\n",
    "@jit\n",
    "def predict(params):\n",
    "  # TODO: Think about the fact that you have to put the hyperparameters here  \n",
    "  mlp = MLP.toplevel([3, 4, 5], variables={'param': params})\n",
    "  return mlp(X)\n",
    "  \n",
    "@jit\n",
    "def loss_fn(params):\n",
    "  Yhat = predict(params)\n",
    "  # TODO: Print in jit\n",
    "  return jnp.mean(jnp.abs(Y - Yhat))\n",
    "\n",
    "@jit\n",
    "def init_params(rng):\n",
    "  # TODO: Think about the fact that you have to put the hyperparameters here  \n",
    "  mlp = MLP.toplevel([3, 4, 5], rngs={'param': rng}, mutable=['param'])\n",
    "  mlp(X)\n",
    "  return mlp.variables()['param']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(1.4570823, dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(init_params(jax.random.PRNGKey(42)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense/0': {'bias': DeviceArray([0.25291714, 0.        , 0.        ], dtype=float32),\n",
       "  'kernel': DeviceArray([[0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ],\n",
       "               [0.25291714, 0.        , 0.        ]], dtype=float32)},\n",
       " 'Dense/1': {'bias': DeviceArray([ 0.04557207, -0.20579326,  0.5184991 ,  0.        ], dtype=float32),\n",
       "  'kernel': DeviceArray([[ 0.08235972, -0.37191805,  0.9370529 ,  0.        ],\n",
       "               [ 0.        , -0.        ,  0.        ,  0.        ],\n",
       "               [ 0.        , -0.        ,  0.        ,  0.        ]],            dtype=float32)},\n",
       " 'Dense/2': {'bias': DeviceArray([-0.2, -0.2, -0.2, -0.2, -0.2], dtype=float32),\n",
       "  'kernel': DeviceArray([[-0.2021582 , -0.2021582 , -0.2021582 , -0.2021582 ,\n",
       "                -0.2021582 ],\n",
       "               [-0.19387028, -0.19387028, -0.19387028, -0.19387028,\n",
       "                -0.19387028],\n",
       "               [-0.23548912, -0.23548912, -0.23548912, -0.23548912,\n",
       "                -0.23548912],\n",
       "               [-0.        , -0.        , -0.        , -0.        ,\n",
       "                -0.        ]], dtype=float32)}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(loss_fn)(init_params(jax.random.PRNGKey(42)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss =  1.4570823 Yhat =  [[-0.65048295 -0.89209783  0.22747914 -0.8393059  -0.13100383]]\n",
      "1 loss =  1.3724773 Yhat =  [[-0.58094865 -0.7692777   0.25747335 -0.68587863 -0.08375458]]\n",
      "2 loss =  1.2976424 Yhat =  [[-0.5199375  -0.6605372   0.28393385 -0.5494852  -0.04218574]]\n",
      "3 loss =  1.2308433 Yhat =  [[-0.46609223 -0.5635818   0.30782127 -0.42721057 -0.00515322]]\n",
      "4 loss =  1.1706475 Yhat =  [[-0.41830716 -0.47650898  0.32992083 -0.31662038  0.02827771]]\n",
      "5 loss =  1.1158575 Yhat =  [[-0.37567115 -0.39772335  0.3508847  -0.21565796  0.058881  ]]\n",
      "6 loss =  1.0654583 Yhat =  [[-0.3374258  -0.32587245  0.37126485 -0.1225654   0.08730697]]\n",
      "7 loss =  1.01858 Yhat =  [[-0.3029324  -0.2597958   0.39153802 -0.03582046  0.11411095]]\n",
      "8 loss =  0.9744629 Yhat =  [[-0.27164698 -0.19848418  0.41212633  0.04591412  0.13977619]]\n",
      "9 loss =  0.93243355 Yhat =  [[-0.24309914 -0.14104642  0.4334131   0.12383318  0.1647319 ]]\n",
      "10 loss =  0.89188176 Yhat =  [[-0.21687557 -0.08668173  0.4557565   0.19902366  0.18936858]]\n",
      "11 loss =  0.8522431 Yhat =  [[-0.19260697 -0.03465696  0.47950214  0.2724954   0.21405075]]\n",
      "12 loss =  0.8129824 Yhat =  [[-0.16995552  0.01571459  0.5049928   0.34520784  0.2391284 ]]\n",
      "13 loss =  0.7735786 Yhat =  [[-0.1486052   0.0650912   0.53257906  0.4180948   0.26494727]]\n",
      "14 loss =  0.7335111 Yhat =  [[-0.12825191  0.11412009  0.5626294   0.49208808  0.29185894]]\n",
      "15 loss =  0.69224566 Yhat =  [[-0.10859458  0.16345426  0.59554034  0.56814075  0.3202309 ]]\n",
      "16 loss =  0.64922005 Yhat =  [[-0.08932619  0.21376987  0.63174784  0.64725125  0.35045698]]\n",
      "17 loss =  0.60382843 Yhat =  [[-0.07012403  0.26578405  0.6717398   0.73048896  0.38296917]]\n",
      "18 loss =  0.5554043 Yhat =  [[-0.05063945  0.32027492  0.7160703   0.81902236  0.41825047]]\n",
      "19 loss =  0.5032006 Yhat =  [[-0.0304857   0.37810406  0.7653772   0.9141513   0.45685035]]\n",
      "20 loss =  0.4533039 Yhat =  [[-0.00922396  0.440243    0.82040274  1.0173447   0.4994034 ]]\n",
      "21 loss =  0.4309895 Yhat =  [[0.01550364 0.46210483 0.847936   1.0037661  0.5232741 ]]\n",
      "22 loss =  0.41142565 Yhat =  [[0.04048632 0.48488075 0.87775767 0.9911665  0.54858065]]\n",
      "23 loss =  0.3881277 Yhat =  [[0.06578112 0.55670345 0.9447177  1.107753   0.5999121 ]]\n",
      "24 loss =  0.3618575 Yhat =  [[0.09341697 0.5834149  0.98066366 1.0965475  0.62976426]]\n",
      "25 loss =  0.34212318 Yhat =  [[0.12192378 0.61176026 1.0197643  1.08631    0.66177446]]\n",
      "26 loss =  0.32251656 Yhat =  [[0.15782161 0.62394416 0.97272    1.0388074  0.67173886]]\n",
      "27 loss =  0.2996928 Yhat =  [[0.18685192 0.65398157 1.0150666  1.0305666  0.7063358 ]]\n",
      "28 loss =  0.28714204 Yhat =  [[0.22153574 0.6666831  0.9716505  0.98671955 0.71770096]]\n",
      "29 loss =  0.2751517 Yhat =  [[0.2512446  0.7449127  1.0530101  1.1016239  0.78271824]]\n",
      "30 loss =  0.24511957 Yhat =  [[0.28479502 0.7549197  1.0101901  1.0509984  0.795876  ]]\n",
      "31 loss =  0.22798 Yhat =  [[0.31760946 0.76586896 0.9708903  1.0043008  0.8100322 ]]\n",
      "32 loss =  0.20246647 Yhat =  [[0.35366696 0.80707824 1.0232872  1.0050415  0.85525113]]\n",
      "33 loss =  0.19412702 Yhat =  [[0.38784522 0.82078844 0.986677   0.9615653  0.8724889 ]]\n",
      "34 loss =  0.1745839 Yhat =  [[0.4208019  0.90355206 1.0889902  1.0677395  0.95945626]]\n",
      "35 loss =  0.14233367 Yhat =  [[0.46075532 0.9234277  1.0510045  1.0261015  0.98125464]]\n",
      "36 loss =  0.11812698 Yhat =  [[0.5004256 0.9439093 1.0159982 0.9858423 1.0048138]]\n",
      "37 loss =  0.10780936 Yhat =  [[0.53051573 0.97743434 0.9926726  1.0205092  0.9808398 ]]\n",
      "38 loss =  0.11313243 Yhat =  [[0.5775589 1.0300927 1.0559261 1.0173239 1.0398784]]\n",
      "39 loss =  0.1388135 Yhat =  [[0.6040769  0.9153841  0.96057457 0.8766802  0.9492167 ]]\n",
      "40 loss =  0.09366264 Yhat =  [[0.6617912 1.025131  1.0551342 1.0108018 1.0390375]]\n",
      "41 loss =  0.11912451 Yhat =  [[0.69142056 0.92219335 0.9548528  0.89015186 0.9457589 ]]\n",
      "42 loss =  0.076585196 Yhat =  [[0.74912   1.0219318 1.0613147 1.0048676 1.043932 ]]\n",
      "43 loss =  0.10120372 Yhat =  [[0.7769967  0.91941136 0.96161956 0.8848545  0.95109934]]\n",
      "44 loss =  0.060259115 Yhat =  [[0.84106094 1.0205176  1.0699041  1.0009768  1.050958  ]]\n",
      "45 loss =  0.080880515 Yhat =  [[0.86758685 0.9180553  0.97049177 0.88118565 0.9582779 ]]\n",
      "46 loss =  0.045232058 Yhat =  [[0.93735576 1.0188435  1.0808176  0.9959657  1.0598207 ]]\n",
      "47 loss =  0.019267917 Yhat =  [[0.9699148 0.9587252 1.0203396 0.997553  1.002193 ]]\n",
      "48 loss =  0.016235663 Yhat =  [[1.0126035 1.0034362 0.9998765 1.045267  0.9802519]]\n",
      "49 loss =  0.02005595 Yhat =  [[0.96742135 0.9561681  1.0180684  0.99442583 1.0002266 ]]\n"
     ]
    }
   ],
   "source": [
    "params = init_params(jax.random.PRNGKey(42))\n",
    "for i in range(50):\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  print(i, \"loss = \", loss, \"Yhat = \", predict(params))\n",
    "  lr = 0.03\n",
    "  params = jax.tree_multimap(lambda x, d: x - lr * d, params, grad)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DenseExplicit(Module):\n",
    "  in_features: int\n",
    "  out_features: int\n",
    "  with_bias: bool = True\n",
    "  kernel_init: Callable = initializers.lecun_normal()\n",
    "  bias_init: Callable = initializers.zeros\n",
    "  name: str = None\n",
    "\n",
    "  def on_attached(self):\n",
    "    self.kernel = self.param('kernel', self.kernel_init, (self.in_features, self.out_features))\n",
    "\n",
    "    if self.with_bias:\n",
    "      self.bias = self.param('bias', self.bias_init, (self.out_features,))\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    x = jnp.dot(x, self.kernel)\n",
    "    if self.with_bias:\n",
    "      x = x + self.bias\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6191777  0.8351118 -0.5551028]\n",
      "[-0.6191777  0.8351118 -0.5551028]\n"
     ]
    }
   ],
   "source": [
    "dense_expl = DenseExplicit.toplevel(\n",
    "  in_features=3, out_features=3,\n",
    "  # TODO: This should have required \n",
    "  rngs={'param': jax.random.PRNGKey(1)}\n",
    ")\n",
    "print(dense_expl(np.ones((3, ))))\n",
    "print(dense_expl(np.ones((3, ))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPExplicit(Module):\n",
    "  def __init__(self, parent, features):\n",
    "    self.parent = parent\n",
    "    self.layers = [\n",
    "      DenseExplicit(self, features[i], features[i+1])\n",
    "      for i in range(len(features)-1)\n",
    "    ]\n",
    "\n",
    "  def on_attached(self):\n",
    "    # NOTE: We can fix the need for this by changing __init__ to:\n",
    "    # self.layers = self.module_list([...]) which would register the\n",
    "    # module lists and then loop over those\n",
    "    for l in self.layers:\n",
    "      l.on_attached()\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for l in self.layers[:-1]:\n",
    "      x = nn.relu(l(x))\n",
    "    return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3998984   0.47382313 -0.79696596  0.68610305 -0.5683795   0.09743354]\n",
      " [-0.3998984   0.47382313 -0.79696596  0.68610305 -0.5683795   0.09743354]\n",
      " [-0.3998984   0.47382313 -0.79696596  0.68610305 -0.5683795   0.09743354]]\n",
      "[[-0.3998984   0.47382313 -0.79696596  0.68610305 -0.5683795   0.09743354]\n",
      " [-0.3998984   0.47382313 -0.79696596  0.68610305 -0.5683795   0.09743354]\n",
      " [-0.3998984   0.47382313 -0.79696596  0.68610305 -0.5683795   0.09743354]]\n"
     ]
    }
   ],
   "source": [
    "mlp_expl = MLPExplicit.toplevel(\n",
    "  features=[3, 4, 5, 6],\n",
    "  rngs={'param': jax.random.PRNGKey(1)}\n",
    ")\n",
    "print(mlp_expl(np.ones((3, 3))))\n",
    "print(mlp_expl(np.ones((3, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_expl.layers[1].bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_weight(module):\n",
    "  @dataclass\n",
    "  class StdWeight(Module):\n",
    "    initialized: bool = False\n",
    "    \n",
    "    def __call__(self, x):\n",
    "      if not self.params():\n",
    "        # initialize parameters\n",
    "        module(x)\n",
    "      \n",
    "      param = module.variables.param\n",
    "      # TODO: Test that I would get an error if I directly modified `param`\n",
    "      param = param.copy(kernel=std(param['kernel']))\n",
    "\n",
    "      def with_vars(variables):\n",
    "        # QUESTION: Can `with_vars` be implemented without assuming\n",
    "        # that modules are dataclasses?\n",
    "        module.__class__.toplevel(\n",
    "          **dataclasses.asdict(module), variables=variables)\n",
    "      return with_vars({'param': param})(x)\n",
    "  return StdWeight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
